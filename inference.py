"""
This module provides an inference pipeline for a lip syncing model. 

Given a .wav audio file, the module extracts MFCC features from the audio 
and feeds them into a trained model to predict the corresponding mouth shapes. 
The module supports both CPU and GPU (CUDA) computation.

The output is a sequence of mouth shapes for each window of audio, which can be 
directly used for animation or further processing.

Functions:
- `preprocess`: Load an audio file and extract MFCC features.
- `postprocess`: Get the most likely mouth shape from the tensor.
- `convert_list_to_RLE`: Convert the list of mouth shapes to a format similar 
  to Run-Length Encoding.
- `convert_to_aegisub_format`: Convert the timestamp-value pairs to aegisub format.
- `infer`: Given a .wav file and a model, load them both and compute the mouth shapes for the .wav.
- `main`: The main function that orchestrates the inference pipeline.

Example usage:

python inference.py --wav_file_name my_audio.wav --model_name my_model.pth
"""

import time
import argparse
import torch
import librosa
from data_classes import LipSyncNet

TARGET_SR = 44100
HOP_LENGTH = 512


def preprocess(filename):
    """
    Load an audio file and extract MFCC features for processing.

    Parameters:
    filename (str): The path to the audio file.

    Returns:
    numpy.ndarray: The MFCC features of the audio file.
    """
    y, sr = librosa.load(filename, sr=None)
    if sr != TARGET_SR:
        y = librosa.resample(y=y, orig_sr=sr, target_sr=TARGET_SR)
    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13, hop_length=HOP_LENGTH)
    mfcc = mfcc.T
    return mfcc


def postprocess(output_data):
    """
    Get the most likely mouth shape from the tensor.

    Parameters:
    output_data (tensor): The data output by the model 
    (In the form of likelihoods of each possible mouth shape for each window.)

    Returns:
    max_likelihood_list: A list with the most likely mouth shape for each window.
    """
    max_likelihood = torch.argmax(
        output_data, dim=2)  # Find the maximum likelihood for each prediction
    max_likelihood_list = torch.flatten(
        max_likelihood).tolist()  # Convert the tensor to a list
    return max_likelihood_list


def convert_list_to_RLE(input_list):
    """
    Converts the list to a format similar to RunLengthEncoding, 
    where instead of a list that gives the mouth shape for every window, 
    it only stores where the mouth shape changes 
    and what it changes to, which is what we need
    This also converts the integers generated by the model 
    into the standard Hannah Barbera mouth shapes. 
    
    
    Parameters: 
    input_list (list) : A list of windows and mouthshapes for each window. 

    Returns:
    timestamps_value_pairs (list) : A list of tuples with timestamps 
    of when the mouth shape changes, 
    and the corresponding new mouth shape. 
    """

    #The length of a single window is 1 second / sample_rate * hop_length
    window_length = 1 / TARGET_SR * HOP_LENGTH
    timestamps = [
        round(index * window_length, 2) for index in range(len(input_list))
    ]

    timestamps_value_pairs = zip(timestamps, input_list)
    timestamps_value_pairs = [
        (t, v) for i, (t, v) in enumerate(timestamps_value_pairs)
        if i == 0 or v != input_list[i - 1]
    ]

    for _, mouth_shape in timestamps_value_pairs:
        if mouth_shape == 7:
            mouth_shape = 'X'
        else:
            mouth_shape = chr(ord('A') + mouth_shape)

    #Now we filter the timestamps
    timestamps_value_pairs = [
        (t, v) for i, (t, v) in enumerate(timestamps_value_pairs)
        if i == 0 or t - timestamps_value_pairs[i - 1][0] > 0.05
    ]
    return timestamps_value_pairs


def convert_to_aegisub_format(timestamps_value_pairs):
    """
    Converts from timestamp_value_pairs format to aegisub format, which is useful for debugging.

    Parameters: 
    timestamps_value_pairs : The times and mouth shapes in timestamp/value pair format.

    Returns:
    A string with all the lines in aegisub .ass format. 
    """
    aegisub_lines = []
    for i in range(len(timestamps_value_pairs)):
        timestamp, text = timestamps_value_pairs[i]
        timestamp_str = '{:.2f}'.format(timestamp)
        if i < len(timestamps_value_pairs) - 1:
            end_timestamp = '{:.2f}'.format(timestamps_value_pairs[i + 1][0])
        else:
            end_timestamp = '{:.2f}'.format(
                timestamp + 1.0)  # Extend the last line for 1 second
        aegisub_line = f'Dialogue: 0,{timestamp_str},{end_timestamp},Default,,0,0,0,,{text}'
        aegisub_lines.append(aegisub_line)
    return '\n'.join(aegisub_lines)


def infer(wav_file_name, model_name):
    '''
    Given a wav_file and a model, loads them both and computes the mouth shapes for the wav.

    Parameters: 
    wav_file_name : Name of a wav file that should be loaded. It should be 41khz.
    model_name : Name of the model you're using. 

    Returns:
    A list of windows and their corresponding mouth shapes.
    '''

    #Due to the small size of the network and input data,
    #this gets much better performance on straight CPU.
    #If your application requires GPU for some reason, simply uncomment the below code.
    ''' 
    if torch.cuda.is_available():
        device = 'cuda'
    else:
        device = 'cpu'
        print("Warning: CUDA not found, using CPU mode.")
    '''
    device = "cpu"
    # Load the trained model
    model_with_params = torch.load(model_name)

    # Extract the parameters
    input_size = model_with_params.input_size
    hidden_size = model_with_params.hidden_size
    output_size = model_with_params.output_size
    num_layers = model_with_params.num_layers

    # Create the model
    model = LipSyncNet(input_size, hidden_size, output_size, num_layers)

    # Load the model weights
    model.load_state_dict(model_with_params.state_dict())
    # Preprocess the input data
    input_data = preprocess(wav_file_name)

    input_data = torch.from_numpy(input_data)
    input_data = input_data.unsqueeze(0)
    #Pass both model and input to device
    model = model.to(device)
    input_data = input_data.to(device)



    # Feed the input data through the model
    start_time = time.time()

    with torch.no_grad():
        output_data = model(input_data)

    elapsed_time = time.time() - start_time
    print(f"Time for inference: {elapsed_time}")
    # Postprocess the output data
    prediction = postprocess(output_data)
    return prediction


def main(wav_file_name, model_name):
    output = infer(wav_file_name=wav_file_name, model_name=model_name)
    rle = convert_list_to_RLE(output)
    int_to_letter = {i: chr(i + 65) for i in range(7)}
    for item1, item2 in rle:
        print(f"{item1}:{int_to_letter[item2]}")


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="Process wav and model name.")
    parser.add_argument(
        '--wav_file_name',
        type=str,
        required=True,
        help='The path to the input data file (Should be a 44khz sampled wav).'
    )
    parser.add_argument('--model_name',
                        type=str,
                        default='model_full_dataset_2layers.pth',
                        help='The name of the model file.')
    args = parser.parse_args()

    main(wav_file_name=args.wav_file_name, model_name=args.model_name)
